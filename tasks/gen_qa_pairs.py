import random
import re

from src.utils import *
from src.api import *
from src.models import *
from tqdm import tqdm
from copy import deepcopy
from typing import Union
import time
import timeit

# TOPICS = {'politicians': 40,
#           'athletes': 20,
#           'sports teams (basketball, soccer, football, baseball .etc)': 20,
#           'sports events (tournaments, leagues, cups .etc)': 20,
#           'countries': 20,
#           'cities': 30,
#           'historical figures': 20,
#           'historical events': 20,
#           'wars': 20,
#           'religions': 10,
#           'singers': 30,
#           'songs': 30,
#           'actors or actresses': 20,
#           'movies or TV series': 20,
#           'writers': 10,
#           'books': 20,
#           'painters': 10,
#           'paintings': 10,
#           'composers': 10,
#           'classical music': 10,
#           'tourist attractions (artificial and natural)': 40,
#           'scientists': 20,
#           'scientific terms': 10,
#           'video games': 10,
#           'animals': 10,
#           'plants': 10,
#           'foods': 10,
#           'enterprises': 20,
#           'international organizations': 20
#           }


# TOPICS = {
#           'politicians': 10,
#           'athletes': 10,
#           'sports teams (basketball, soccer, football, baseball .etc)': 10,
#           'sports events (tournaments, leagues, cups .etc)': 10,
#           'countries': 10,
#           'cities': 10,
#           'historical figures': 10,
#           'historical events': 10,
#           'wars': 10,
#           'religions': 10,
#           'singers': 10,
#           'songs': 10,
#           'actors or actresses': 10,
#           'movies or TV series': 10,
#           'writers': 10,
#           'books': 10,
#           'painters': 10,
#           'paintings': 10,
#           'composers': 10,
#           'classical music': 10,
#           'tourist attractions (artificial and natural)': 10,
#           'scientists': 20,
#           'scientific terms': 10,
#           'video games': 10,
#           'animals': 10,
#           'plants': 10,
#           'foods': 10,
#           'enterprises': 10,
#           'international organizations': 10
#           }

TOPICS = {

    'newzealand transport agency and newzealand transport related topics (road, projects, safety, vehicles, personel, organization, etc)': 100,
}


def list_topic_terms(model: Union[APIModel, LocalModel], topic: str, already_generated = [] ):
    # prompt = f"List some {topic}, separated by '|':\n"
    prompt = f"List some {topic} separated by '|', for example <topic1> | <topic2> | <topic3> , etc, in that exact format. Do not include terms alredy found in this list {already_generated}:"
    print(f"List some {topic} separated by |:")
    response = model.get_response(prompt, temperature=.5, max_tokens=64)
    print(f"Response: {response}")
    terms = [it.strip() for it in response.split('|')]
    return terms


def generate_passage_for_term(
        model: Union[APIModel, LocalModel],
        term: str,
        max_tokens: int = 256,
        temperature: float = 0.0,
        template: Dict = None,
        detailed: bool = False
):
    if template:
        example_term, example_passage = template["term"], template["passage"]
        prompt = f"Generate a passage in Wikipedia style about {example_term}:\n{example_passage}\n\n Keep it relate d to to transport in new zealand as a broad category. dont worry if your infomration is outdated do not mention this. dont worry if you dont have direct access to wikipedia do not mentino this either. it is ok, just write the passage to the best of your ability without refering to this message on the paragraph. DO NOT SAY 'I''m sorry, but I cannot provide verbatim excerpts from Wikipedia':"
    else:
        prompt = ""

    if detailed:
        prompt += f"Generate a detailed passage from Wikipedia style about {term}. Keep it related to transport in new zealand as a broad category:"
    else:
        prompt += f"Generate a passage in Wikipedia style about {term}. keep it related to transport iin new zealand as a broad category:"
    raw_passage = model.get_response(prompt, temperature=temperature, max_tokens=max_tokens)

    # remove the incomplete sentence at the end
    raw_sentences = nltk.sent_tokenize(raw_passage)
    sentences = deepcopy(raw_sentences[:-1]) if not raw_sentences[-1].endswith('.') else deepcopy(raw_sentences)
    passage = ' '.join(sentences)

    return passage


def extract_entities_in_passage(
        model: APIModel,
        passage: str,
        max_tokens: int = 128,
        extractor: str = "spacy-nltk",
        ban_entity_list: List[str] = None,
):
    # extract entities
    all_entities = []
    if "spacy" in extractor:
        doc = nlp(passage)
        entities_spacy = [clean_entity(ent.text) for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'GPE', 'LOC', 'DATE']]
        entities_spacy = [ent for ent in entities_spacy if ent is not None]
        all_entities += entities_spacy

    # entity_source_2
    if "llm" in extractor:
        prompt_extract_entity = f"Passage:\n{passage}\n\n" \
                                f"Extract the named entities (like date, location, organization, character, number, people, streets, projects) in the above passage, entities should be separated by '|'. for example <entity1> | <entity2> | <entity3> , etc, in that exact format:" \
                                f"If no named entity in it, return 'None' only. Select entities that are part of the  subject of transport in newzealand."
        raw_entities_by_llm = model.get_response(prompt_extract_entity, temperature=0.0, max_tokens=max_tokens)

        entities_llm = [clean_entity(ent) for ent in raw_entities_by_llm.split('|')]
        entities_llm = [ent for ent in entities_llm if ent is not None]
        all_entities += entities_llm

    # entity_source_3
    if "nltk" in extractor:
        entities_nltk = process_an_item(passage)
        entities_nltk = [clean_entity(ent) for ent in entities_nltk]
        entities_nltk = [ent for ent in entities_nltk if ent is not None]
        all_entities += entities_nltk

    entities = [[], []]
    for rent in all_entities:
        if rent.endswith('He'):
            rent = rent[:-len('He')]
        if rent.endswith('She'):
            rent = rent[:-len('She')]
        if rent.endswith('They'):
            rent = rent[:-len('They')]
        if rent.endswith('It'):
            rent = rent[:-len('It')]

        if normalize_answer(rent) not in entities[1]:
            entities[0].append(rent)
            entities[1].append(normalize_answer(rent))

    # clean entities[0], so that sub_strings are removed
    final_entities = [[], []]
    for idx, item in enumerate(entities[0]):
        addf = True
        for another_item in entities[0][:idx] + entities[0][idx + 1:]:
            if normalize_answer(item) in normalize_answer(another_item):
                addf = False
                break
        if addf:
            only_lower_case = True
            for char in item:
                if char not in whitespace + ascii_lowercase:
                    only_lower_case = False
                    break
            if only_lower_case:
                final_entities[1].append(item)
            else:
                final_entities[0].append(item)

    random.shuffle(final_entities[0])
    random.shuffle(final_entities[1])

    # 1 are the entities that contain only lowercase, otherwise 0
    final_entities = final_entities[0] + final_entities[1]
    # lemmatize the entities
    lemma_final_entities = [lemmatize(ent) for ent in final_entities]

    output_entities = []
    if ban_entity_list:
        for ent in final_entities:
            ban_flag = False
            for ban_entity in ban_entity_list:
                if normalize_answer(ban_entity) in normalize_answer(ent):
                    ban_flag = True
                    break

                if ban_entity in ent:
                    ban_flag = True
                    break

                if lemmatize(ban_entity) in lemmatize(ent):
                    ban_flag = True
                    break

            if not ban_flag:
                output_entities.append(ent)

    return output_entities


def generate_qae_pair_for_entity(
        model: APIModel,
        passage: str,
        title: str,
        expected_answer: str,
        question_tokens: int = 64,
        answer_tokens: int = 10,
        explanation_tokens: int = 128,
        question_temperature: float = 0.0,
        max_retry: int = 10,
        title_in_question: bool = False,
        template: Dict = None,
        ban_entity_list: List[str] = None,
):
    if template:
        template_passage = template["passage"]
        template_question = template["question"]
        template_answer = template["answer"]
        template_explanation = template["explanation"]
        prompt_generate_question = f"Passage:\n{template_passage}\n\nEntity: {template_answer}\n\n" \
                                   f"Question:\n{template_question}\n\n"
        prompt_generate_answer = f"Passage:\n{template_passage}\n\nQuestion:\n{template_question}\n\nAnswer:\n{template_answer}\n\n"
        prompt_generate_explanation = f"Passage:\n{template_passage}\n\nQuestion:\n{template_question}\n\nAnswer:\n{template_answer}\n\n" \
                                      f"Explanation:\n{template_explanation}\n\n"
    else:
        prompt_generate_question = ""
        prompt_generate_answer = ""
        prompt_generate_explanation = ""

    # generate question
    if title_in_question:
        prompt_generate_question += f"Passage about {title}:\n{passage}\n\n" \
                                    f"Generate a question that meets the following conditions: 1. contains the term '{title}' in question, 2. the answer is '{expected_answer}'."
    else:
        prompt_generate_question += f"Passage about {title}:\n{passage}\n\n" \
                                    f"Generate a question to which the answer is the entity '{expected_answer}'."

    if ban_entity_list:
        prompt_generate_question += f"(Avoid the following entities in the question: {', '.join(ban_entity_list)})."

    question = model.get_response(prompt_generate_question, temperature=question_temperature,
                                  max_tokens=question_tokens,
                                  ban_pronoun=True)

    if title_in_question:
        num_retry = 0
        while not normalize_answer(title.lower()) in normalize_answer(question.lower()):
            if num_retry > max_retry:
                # print(f"Failed to generate question that contains the title: Q=[{question}], title=[{title}].")
                return None
            question = model.get_response(prompt_generate_question, temperature=question_temperature,
                                          max_tokens=question_tokens,
                                          ban_pronoun=True)
            num_retry += 1

    prompt_generate_answer += f"Passage about {title}:\n{passage}\n\nQuestion:\n{question}\n\nExtract the answer directly from the passage in less words as possible."
    raw_answer = model.get_response(prompt_generate_answer, temperature=0.0, max_tokens=answer_tokens)

    num_retry = 0

    while normalize_answer(raw_answer) != normalize_answer(expected_answer):
        if num_retry > max_retry:
            # print(f"Failed to generate expected answer: {expected_answer}, raw answer: {raw_answer}")
            return None
        raw_answer = model.get_response(prompt_generate_answer, temperature=0.0, max_tokens=answer_tokens)
        num_retry += 1

    answer = raw_answer

    # prompt_generate_explanation = f"Passage:\n{passage}\n\nQuestion:\n{question}\n\nAnswer:\n{answer}\n\n" \
    #                            f"Refer to the passage and extract an evidence sentence as explanation, " \
    #                            f"{answer} must in the explanation."

    prompt_generate_explanation += f"Passage about {title}:\n{passage}\n\nQuestion:\n{question}\n\nAnswer:\n{answer}\n\n" \
                                   f"According to an evidence from the passage to support the answer, rewrite it to make its meaning clear without passage."

    # prompt_generate_explanation += f"Passage:\n{passage}\n\nQuestion:\n{question}\n\nAnswer:\n{answer}\n\n" \
    #                                f"According to the passage, write a short explanation for the answer."

    explanation = model.get_response(prompt_generate_explanation, temperature=0.0, max_tokens=explanation_tokens,
                                     ban_pronoun=True)

    # num_retry = 0

    # while answer not in explanation:
    #     explanation = model.get_response(prompt_generate_explanation, temperature=0.5, max_tokens=explanation_tokens,
    #                                      ban_pronoun=True)
    #     if num_retry > max_retry:
    #         print(f"Failed to generate expected explanation: {explanation}, raw answer: {answer}")
    #         return None
    #     num_retry += 1

    return {
        "question": question,
        "answer": answer,
        "explanation": explanation
    }


def generate_qa_pairs_for_passage(
        model: APIModel,
        passage: str,
        title: str = "",
        entity_tokens: int = 64,
        question_tokens: int = 64,
        answer_tokens: int = 10,
        explanation_tokens: int = 128,
        extractor: str = "nltk-spacy",
        max_qae_pairs: int = 10
):
    # extract entities
    entities = extract_entities_in_passage(model, passage, max_tokens=entity_tokens, extractor=extractor)

    # generate question-answer pairs
    qae_pairs = []
    normalized_questions = []
    for entity in entities:
        if len(entity.split()) > 5:
            continue

        qae_pair = generate_qae_pair_for_entity(
            model=model,
            passage=passage,
            title=title,
            expected_answer=entity,
            question_tokens=question_tokens,
            answer_tokens=answer_tokens,
            explanation_tokens=explanation_tokens
        )
        if qae_pair is None:
            continue

        duplicated_question = False
        for q in normalized_questions:
            if normalize_answer(qae_pair["question"]) == q:
                duplicated_question = True
                break
        if duplicated_question:
            continue
        else:
            normalized_questions.append(normalize_answer(qae_pair["question"]))
            qae_pairs.append(qae_pair)

        if len(qae_pairs) >= max_qae_pairs:
            break

    return qae_pairs


def generate_examples_for_topic(
        model: APIModel,
        topic: str,
        num_terms: int,
        passage_tokens: int = 256,
        entity_tokens: int = 64,
        question_tokens: int = 64,
        answer_tokens: int = 10,
        explanation_tokens: int = 128,
        extractor: str = "nltk-spacy",
        max_qae_pairs: int = 10,
        topic_cate: str = None,
):
    # collect terms
    all_terms = []
    while len(all_terms) < num_terms:
        terms = list_topic_terms(model, topic)
        for term in terms:
            if term not in all_terms:
                all_terms.append(term)
        print(f"{topic} has collected {len(all_terms)} terms.")

    all_terms = all_terms[:num_terms]
    print(f"{topic} has collected {len(all_terms)} terms, Done.")

    topic_qa_pairs = []

    # generate passages
    for term in tqdm(all_terms, desc=f"Generating passages for {topic}"):
        passage = generate_passage_for_term(
            model,
            term=term,
            max_tokens=passage_tokens,
        )

        # generate question-answer pairs
        qae_pairs = generate_qa_pairs_for_passage(
            model,
            passage=passage,
            title=term,
            entity_tokens=entity_tokens,
            question_tokens=question_tokens,
            answer_tokens=answer_tokens,
            explanation_tokens=explanation_tokens,
            extractor=extractor,
            max_qae_pairs=max_qae_pairs
        )

        for qae_pair in qae_pairs:
            topic_qa_pairs.append({
                "topic": topic,
                "term": term,
                "passage": passage,
                "question": qae_pair["question"],
                "answer": qae_pair["answer"],
                "explanation": qae_pair["explanation"]
            })

    return topic_qa_pairs


# self prompt cot code
def generate_base_quadruples(
        model: APIModel,
        term: str,
        passage_tokens: int = 512,
        question_tokens: int = 64,
        answer_tokens: int = 10,
        explanation_tokens: int = 128,
        num_questions_per_paragraph: int = 2,
        entity_extractor: str = "nltk-spacy"
):
    # generate long and detailed passage
    passage = generate_passage_for_term(
        model=model,
        term=term,
        max_tokens=passage_tokens,
        detailed=True
    )

    paragraphs = []
    sentences = nltk.sent_tokenize(passage)

    for i in range(0, len(sentences), 2):
        paragraphs.append(" ".join(sentences[i:i + 2]))
    if len(sentences) % 2 != 0:
        paragraphs[-1] = paragraphs[-1] + " " + sentences[-1]
    existing_questions = []
    existing_answers = []
    output = {
        "term": term,
        "passage": passage,
        "qae_pairs": []
    }

    # generate question-answer pairs for each paragraph
    for paragraph in tqdm(paragraphs):
        num_retry = 0
        qae_pairs = []

        entities = extract_entities_in_passage(
            model=model,
            passage=paragraph,
            max_tokens=64,
            extractor=entity_extractor,
        )

        for entity in entities:
            qae_pair = generate_qae_pair_for_entity(
                model=model,
                passage=paragraph,
                title=term,
                expected_answer=entity,
                question_tokens=question_tokens,
                answer_tokens=answer_tokens,
                explanation_tokens=explanation_tokens
            )
            if qae_pair is None:
                continue

            if normalize_answer(qae_pair["question"]) in existing_questions:
                continue

            if normalize_answer(qae_pair["answer"]) in existing_answers:
                continue

            existing_questions.append(normalize_answer(qae_pair["question"]))
            existing_answers.append(normalize_answer(qae_pair["answer"]))
            qae_pairs.append(qae_pair)

            if len(qae_pairs) >= num_questions_per_paragraph:
                break

        for qae_pair in qae_pairs:
            output["qae_pairs"].append({
                "context": paragraph,
                "question": qae_pair["question"],
                "answer": qae_pair["answer"],
                "explanation": qae_pair["explanation"]
            })

    return output


def extract_entities_and_labels_spacy(
        passage: str,
        ban_entity_list: List[str] = None,
):
    doc = nlp(passage)
    entities = [clean_entity(ent.text) for ent in doc.ents]
    labels = [ent.label_ for ent in doc.ents]

    if ban_entity_list:
        final_entities, final_labels = [], []
        lemma_final_entities = [lemmatize(ent) for ent in entities]
        lemma_ban_entity_list = [lemmatize(ent) for ent in ban_entity_list]
        block_entities = [normalize_answer(ent) for ent in lemma_ban_entity_list]
        for i in range(len(entities)):
            if normalize_answer(lemma_final_entities[i]) not in block_entities:
                final_entities.append(entities[i])
                final_labels.append(labels[i])
        entities = final_entities
        labels = final_labels

    return entities, labels


def generate_multi_hop_quadruples(
        model: APIModel,
        term: str,
        passage_tokens: int = 512,
        question_tokens: int = 64,
        answer_tokens: int = 10,
        explanation_tokens: int = 128,
        num_entity_per_paragraph: int = 2,
        min_passage_tokens: int = 128,
        entity_extractor: str = "spacy",
        question_ban_list: List[str] = None,
        answer_ban_list: List[str] = None,
        detailed: bool = False,
        title_in_question: bool = False,
):
    # filter out terms like November 2020
    if re.match(DATE_PATTERN, term.lower()) is not None:
        # print(f"Term {term} is a date, skip it.")
        return None

    # filter out terms that are numbers
    if re.match(r"\d+$", normalize_answer(term)) is not None:
        # print(f"Term {term} is a number, skip it.")
        return None

    for e in ["million", "trillion", "billion", "hundred", "thousand"]:
        if e in term:
            return None

    # refuse to generate passage for [QUANTITY, ORDINAL, CARDINAL, PERCENT, MONEY, DATE, TIME]
    input_entities = nlp(term)
    for ent in input_entities.ents:
        if ent.label_ in ["QUANTITY", "ORDINAL", "CARDINAL", "PERCENT", "MONEY", "DATE", "TIME"]:
            # print(f"Term {term} is a {ent.label_}, skip it.")
            return None

    # generate long and detailed passage
    passage = generate_passage_for_term(
        model=model,
        term=term,
        max_tokens=passage_tokens,
        detailed=detailed
    )

    if len(passage.split()) < min_passage_tokens:
        # print(f"Passage for {term} is too short, skip it. (len: {len(passage.split())})")
        return None

    paragraphs = []
    sentences = nltk.sent_tokenize(passage)

    for i in range(0, len(sentences), 2):
        paragraphs.append(" ".join(sentences[i:i + 2]))
    if len(sentences) % 2 != 0:
        paragraphs[-1] = paragraphs[-1] + " " + sentences[-1]

    existing_questions = []
    existing_answers = []
    existing_invalid_answers = []
    output = {
        "term": term,
        "passage": passage,
        "qae_pairs": []
    }

    # generate question-answer pairs for each paragraph
    # for paragraph in tqdm(paragraphs, desc=f"Generate QA pairs for {term}"):
    print(f"Generate QA pairs for {term}.......")
    for i, paragraph in enumerate(paragraphs):
        print(f"\tProgress: {i + 1}/{len(paragraphs)}")
        qae_pairs = []

        # We should avoid generating answers for the same entity in previous answers
        entities = extract_entities_in_passage(
            model=model,
            passage=paragraph,
            extractor=entity_extractor,
            ban_entity_list=answer_ban_list
        )
        #
        # # entities = [ent for ent in entities if ent not in term]
        # entities, labels = extract_entities_and_labels_spacy(
        #     passage=paragraph,
        #     ban_entity_list=answer_ban_list
        # )

        random.shuffle(entities)

        processed_cnt = 0
        for entity in entities:
            if entity in existing_invalid_answers:
                continue
            if processed_cnt >= num_entity_per_paragraph:
                break
            processed_cnt += 1
            # We should avoid generating questions containing previous answers in question text
            t1 = time.time()
            qae_pair = generate_qae_pair_for_entity(
                model=model,
                passage=paragraph,
                title=term,
                expected_answer=entity,
                question_tokens=question_tokens,
                answer_tokens=answer_tokens,
                explanation_tokens=explanation_tokens,
                ban_entity_list=question_ban_list,
                title_in_question=title_in_question
            )
            t2 = time.time()

            if qae_pair is None:
                # print(f"Generate qae pair for {entity} takes {t2 - t1} seconds. False.")
                existing_invalid_answers.append(entity)
                continue
            # print(f"Generate qae pair for {entity} takes {t2 - t1} seconds.True.")

            if normalize_answer(qae_pair["question"]) in existing_questions:
                continue

            if normalize_answer(qae_pair["answer"]) in existing_answers:
                existing_invalid_answers.append(entity)
                continue

            existing_questions.append(normalize_answer(qae_pair["question"]))
            existing_answers.append(normalize_answer(qae_pair["answer"]))
            qae_pairs.append(qae_pair)

        for qae_pair in qae_pairs:
            output["qae_pairs"].append({
                "context": paragraph,
                "question": qae_pair["question"],
                "answer": qae_pair["answer"],
                "explanation": qae_pair["explanation"]
            })

    return output


def generate_quadruples_for_term(
        model: APIModel,
        term: str,
        passage_tokens: int = 512,
        question_tokens: int = 64,
        answer_tokens: int = 10,
        explanation_tokens: int = 128,
        num_questions_per_paragraph: int = 1,
        max_retry: int = 10,
):
    # generate long and detailed passage
    passage = generate_passage_for_term(
        model=model,
        term=term,
        max_tokens=passage_tokens,
        detailed=True
    )

    paragraphs = []
    sentences = nltk.sent_tokenize(passage)

    for i in range(0, len(sentences), 2):
        paragraphs.append(" ".join(sentences[i:i + 2]))
    if len(sentences) % 2 != 0:
        paragraphs[-1] = paragraphs[-1] + " " + sentences[-1]
    existing_questions = []
    output = {
        "term": term,
        "passage": passage,
        "qae_pairs": []
    }

    # generate question-answer pairs for each paragraph
    for paragraph in tqdm(paragraphs):
        num_retry = 0
        qae_pairs = []
        while len(qae_pairs) < num_questions_per_paragraph and num_retry < max_retry:
            qae_pair = generate_qae_pair_for_entity(
                model=model,
                passage=paragraph,
                title=term,
                expected_answer=term,
                question_tokens=question_tokens,
                answer_tokens=answer_tokens,
                explanation_tokens=explanation_tokens
            )
            if qae_pair is None:
                num_retry += 1
                continue

            if normalize_answer(qae_pair["question"]) not in existing_questions:
                existing_questions.append(normalize_answer(qae_pair["question"]))
                qae_pairs.append(qae_pair)
            else:
                num_retry += 1

            if num_retry > 10:
                break

        for qae_pair in qae_pairs:
            output["qae_pairs"].append({
                "context": paragraph,
                "question": qae_pair["question"],
                "answer": qae_pair["answer"],
                "explanation": qae_pair["explanation"]
            })

    return output


#
# def generate_hop_quadruples_for_question(
#         model: APIModel,
#         base_answer: str,
#         base_question: str,
#         passage_tokens: int = 512,
#         question_tokens: int = 64,
#         answer_tokens: int = 10,
#         explanation_tokens: int = 128,
#         num_questions_per_paragraph: int = 1,
#         max_retry: int = 10,
#         ban_entity_list: List[str] = None,
#         ban_entity_types: List[str] = None,
# ):
#
#     # generate long and detailed passage
#     passage = generate_passage_for_term(
#         model=model,
#         term=base_answer,
#         max_tokens=passage_tokens,
#         detailed=False
#     )
#
#     raw_paragraphs = []
#     sentences = nltk.sent_tokenize(passage)
#
#     for i in range(0, len(sentences), 2):
#         raw_paragraphs.append(" ".join(sentences[i:i + 2]))
#     if len(sentences) % 2 != 0:
#         raw_paragraphs[-1] = raw_paragraphs[-1] + " " + sentences[-1]
#
#     existing_questions = [normalize_answer(base_question)]
#
#     output = {
#         "term": base_answer,
#         "passage": passage,
#         "qae_pairs": []
#     }
#
#     # generate question-answer pairs for each paragraph
#     for paragraph in tqdm(raw_paragraphs):
#         num_retry = 0
#         qae_pairs = []
#         while len(qae_pairs) < num_questions_per_paragraph and num_retry < max_retry:
#             qae_pair = generate_qae_pair_for_entity(
#                 model=model,
#                 passage=paragraph,
#                 title=base_answer,
#                 expected_answer=term,
#                 question_tokens=question_tokens,
#                 answer_tokens=answer_tokens,
#                 explanation_tokens=explanation_tokens,
#                 ban_entity_list=q_entity
#             )
#             if qae_pair is None:
#                 num_retry += 1
#                 continue
#
#             if normalize_answer(qae_pair["question"]) not in existing_questions:
#                 existing_questions.append(normalize_answer(qae_pair["question"]))
#                 qae_pairs.append(qae_pair)
#             else:
#                 num_retry += 1
#
#             if num_retry > 10:
#                 break
#
#         for qae_pair in qae_pairs:
#             output["qae_pairs"].append({
#                 "context": paragraph,
#                 "question": qae_pair["question"],
#                 "answer": qae_pair["answer"],
#                 "explanation": qae_pair["explanation"]
#             })
#
#     return output
#
#


def parse_arguments():
    parser = argparse.ArgumentParser(description="Zero-shot-CoT")

    # Default parameters
    parser.add_argument("--task", type=str, default="self-prompt-cot",
                        choices=["self-prompt", "self-prompt-cot"],
                        help="task name in [open-domain-qa, fact-verification]")

    # parser.add_argument("--method", type=str, default="2-hop",
    #                     choices=["2-hop", "3-hop-linear", "3-hop-tree", "4-hop-linear", "4-hop-tree-balance", "4-hop-tree-unbalance"],
    #                     help="method name in [2-hop, 3-hop-linear, 3-hop-tree, 4-hop-linear, 4-hop-tree-balance, 4-hop-tree-unbalance]")

    parser.add_argument(
        "--model_name", type=str, default="gpt-4o-mini", help="model used for response generation.")

    parser.add_argument("--output_path", type=str, default="data/self-prompt-cot/")
    parser.add_argument("--output_file", type=str, default=None)

    parser.add_argument("--random_seed", type=int, default=42)

    parser.add_argument(
        "--passage_tokens", type=int, default=512, help="maximum length of output tokens by model for zero-shot")
    parser.add_argument(
        "--question_tokens", type=int, default=64, help="maximum length of output tokens by model for zero-shot")
    parser.add_argument(
        "--answer_tokens", type=int, default=10, help="maximum length of output tokens by model for zero-shot")
    parser.add_argument(
        "--explanation_tokens", type=int, default=128, help="maximum length of output tokens by model for zero-shot")
    parser.add_argument(
        "--min_passage_tokens", type=int, default=80, help="maximum length of output tokens by model for zero-shot")

    parser.add_argument(
        "--topic", type=str, default="politician", help="topic name for zero-shot")

    parser.add_argument(
        "--terms_per_topic", type=int, default=10, help="number of terms per topic for zero-shot")

    parser.add_argument(
        "--num_qae_pairs_per_term", type=str, default=5, help="number of qae pairs per term for zero-shot")

    parser.add_argument("--max_terms", type=int, default=10, help="maximum number of terms to generate")

    parser.add_argument("--topic_index", type=int, default=-1, help="index of topic to generate")
    parser.add_argument("--log", type=str, default="", help="index of topic to generate")
    parser.add_argument("--log_dir", type=str, default="", help="index of topic to generate")

    args = parser.parse_args()

    if args.topic_index >= 0:
        args.topic = list(TOPICS.keys())[args.topic_index]
        args.max_terms = TOPICS[args.topic]
        print(f"topic: {args.topic}, max_terms: {args.max_terms}")

    # get base dir of args.output_path
    args.output_path = os.path.join(args.output_path, args.model_name)
    os.makedirs(args.output_path, exist_ok=True)
    output_file_name = f"{args.topic}_{args.max_terms}.json"
    args.output_file = os.path.join(args.output_path, output_file_name)

    if args.log_dir == "":
        args.log_dir = f"logs/{args.model_name}"
    os.makedirs(args.log_dir, exist_ok=True)
    args.log = os.path.join(args.log_dir, f"{args.topic}_{args.max_terms}.log")

    print(f"output file: {args.output_file}")
    print(f"log file: {args.log}")

    return args


def main(args):
    fix_seed(args.random_seed)
    logger = open(args.log, "a")

    # init model
    if args.model_name in ["gpt-3.5-turbo-0301", "gpt-3.5-turbo", "gpt-3.5-turbo-0125", "gpt-4-turbo", "gpt-4o-mini"]:
        model = ChatGPT(model_name=args.model_name)
    elif args.model_name in ["text-davinci-002", "text-davinci-003"]:
        model = CompleteGPT(model_name=args.model_name)
    elif args.model_name in ["gpt-neoxt-chat"]:
        model = GPTNeoXTChat(model_name=args.model_name)
    elif args.model_name in ["flan-ul2", "flan-t5-xxl"]:
        model = Flan_UL2_20B(model_name=args.model_name)
    elif args.model_name in [
        "t5-11b-ssm", "t5-xxl-ssm", "t5-large-ssm"
    ]:
        model = T5_SSM(model_name=args.model_name)
    elif args.model_name in ["alpaca-7b"]:
        model = Alpaca(model_name=args.model_name)
    elif args.model_name in ["falcon-7b"]:
        model = Falcon(model_name=args.model_name)
    else:
        raise ValueError(f"model {args.model_name} not supported.")

    all_terms = []
    # dd a random call to the model to see if it is working
    
    while len(all_terms) < args.max_terms:
        terms = list_topic_terms(model, args.topic, already_generated=all_terms)
        print(f"collected {len(all_terms)}/ {args.max_terms}")
        for term in terms:
            if term not in all_terms:
                all_terms.append(term)
            if len(all_terms) >= args.max_terms:
                break
    print(f"collected {len(all_terms)} terms: {all_terms}")
    logger.write(f"collected {len(all_terms)} terms: {all_terms}\n")

    # generate base quadruples
    first_hops = {}
    second_hops = {}
    dataset = {}

    print("========First Hop=========")
    logger.write("========First Hop=========\n")
    print_now()
    for i, term in enumerate(all_terms):
        print(f"[{print_now(1)}] Processing term [{term}], progress {i + 1}/{len(all_terms)}")
        logger.write(f"[{print_now(1)}] Processing term [{term}], progress {i + 1}/{len(all_terms)}\n")
        output = generate_multi_hop_quadruples(
            model=model,
            term=term,
            passage_tokens=args.passage_tokens,
            question_tokens=args.question_tokens,
            answer_tokens=args.answer_tokens,
            explanation_tokens=args.explanation_tokens,
            min_passage_tokens=args.min_passage_tokens,
            num_entity_per_paragraph=3,
            answer_ban_list=[term],
            detailed=True,
            title_in_question=False
        )
        if output and len(output["qae_pairs"]) > 0:
            print("This is the output")
            print(output)
            output["hops"] = [term]
            first_hops[term] = output
    print(f"this are the first hops")
    print(first_hops)
    dataset["first_hops"] = first_hops
    with open(args.output_file, "w") as f:
        json.dump(dataset, f, indent=4, ensure_ascii=False)

    print("\n\n\n========Second Hop=========")
    logger.write("\n\n\n========Second Hop=========\n")
    print_now()
    for j, term in enumerate(first_hops):
        print(f"[{print_now(1)}] Processing [{term}] qae_pairs, progress {j}/{len(first_hops)}...")
        logger.write(f"[{print_now(1)}] Processing [{term}] qae_pairs, progress {j}/{len(first_hops)}...\n")
        for i, qae_pair in enumerate(first_hops[term]["qae_pairs"]):
            print(
                f"[{print_now(1)}] generating second hop for {qae_pair['answer']}, progress {i}/{len(first_hops[term]['qae_pairs'])}...")
            logger.write(
                f"[{print_now(1)}] generating second hop for {qae_pair['answer']}, progress {i}/{len(first_hops[term]['qae_pairs'])}...\n")
            if qae_pair["answer"] in first_hops:
                output = first_hops[qae_pair["answer"]]
            else:
                output = generate_multi_hop_quadruples(
                    model=model,
                    term=qae_pair["answer"],
                    passage_tokens=args.passage_tokens,
                    question_tokens=args.question_tokens,
                    answer_tokens=args.answer_tokens,
                    explanation_tokens=args.explanation_tokens,
                    min_passage_tokens=args.min_passage_tokens,
                    num_entity_per_paragraph=3,
                    answer_ban_list=first_hops[term]["hops"] + [qae_pair["answer"]],
                    question_ban_list=first_hops[term]["hops"],
                    detailed=False,
                    title_in_question=True
                )
            if output and len(output["qae_pairs"]) > 0:
                output["hops"] = first_hops[term]["hops"] + [qae_pair["answer"]]
                second_hops[qae_pair["answer"]] = output
            # else:
            #     print(f"failed to generate second hop for {qae_pair['answer']}")

    dataset["second_hops"] = second_hops
    with open(args.output_file, "w") as f:
        json.dump(dataset, f, indent=4, ensure_ascii=False)

    print("\n\n\n========End=========")
    logger.write("\n\n\n========End=========\n")
    logger.close()


if __name__ == "__main__":
    args = parse_arguments()
    print(f"Running main with args: {args}")
    #make a random call to openai mmodel to see if it is working
    
    main(args)
    # print("Done!")

